<!DOCTYPE html>
<html lang="fr" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Contexte - Deep Learning</title>

    <link rel="stylesheet" href="../../../../../lib/theme.css">
    <link rel="stylesheet" href="../../_shared/deep-learning.css">
    <script src="https://cdn.tailwindcss.com"></script>

    <script type="module">
        import { initSlide, sendTOC, setupScrollHandler } from '../../../../../parcours/_shared/slide-utils.js';
        initSlide();
        setupScrollHandler();
        sendTOC([
            { id: 'history', label: 'Un Peu d\'Histoire', icon: 'üìú' },
            { id: 'pitfalls', label: 'Pi√®ges Classiques', icon: '‚ö†Ô∏è' },
        ]);
    </script>
</head>
<body class="antialiased selection:bg-blue-500 selection:text-white">

    <main class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-16">

        <!-- HISTOIRE -->
        <section id="history" class="mb-24 scroll-mt-24">
            <div class="flex items-center gap-3 mb-6">
                <span class="text-3xl">üìú</span>
                <h2 class="text-3xl font-bold dl-text-primary">Un Peu d'Histoire</h2>
            </div>

            <div class="prose-content dl-text-secondary">
                <p class="mb-8">
                    La r√©tropropagation n'est pas une invention soudaine, mais le fruit de d√©cennies de recherche en math√©matiques, th√©orie du contr√¥le, et informatique.
                </p>

                <div class="space-y-6">
                    <div class="history-card">
                        <span class="text-cyan-400 font-mono text-xs mb-1 block">1960s - A√©rospatiale</span>
                        <h3 class="text-lg font-semibold dl-text-primary">Kelley & Bryson</h3>
                        <p class="dl-text-muted mt-1 text-sm">Utilisent les gradients pour optimiser les trajectoires de fus√©es. Les bases math√©matiques existent d√©j√† !</p>
                    </div>
                    <div class="history-card">
                        <span class="text-cyan-400 font-mono text-xs mb-1 block">1970 - Diff√©rentiation Automatique</span>
                        <h3 class="text-lg font-semibold dl-text-primary">Seppo Linnainmaa</h3>
                        <p class="dl-text-muted mt-1 text-sm">Formalise le "mode inverse" de la diff√©rentiation automatique. C'est l'algorithme qui rend le calcul efficace.</p>
                    </div>
                    <div class="history-card">
                        <span class="text-cyan-400 font-mono text-xs mb-1 block">1974 - La Th√®se Oubli√©e</span>
                        <h3 class="text-lg font-semibold dl-text-primary">Paul Werbos</h3>
                        <p class="dl-text-muted mt-1 text-sm">Dans sa th√®se de PhD, il propose explicitement d'appliquer cela aux r√©seaux de neurones. Malheureusement, c'est l'√©poque de "l'hiver de l'IA" et personne n'y pr√™te attention.</p>
                    </div>
                    <div class="history-card">
                        <span class="text-cyan-400 font-mono text-xs mb-1 block">1986 - Le Triomphe</span>
                        <h3 class="text-lg font-semibold dl-text-primary">Rumelhart, Hinton & Williams</h3>
                        <p class="dl-text-muted mt-1 text-sm">Leur article dans <em>Nature</em> popularise enfin la technique. Ils montrent qu'un r√©seau peut apprendre des repr√©sentations internes, r√©solvant le fameux probl√®me XOR.</p>
                    </div>
                    <div class="history-card">
                        <span class="text-cyan-400 font-mono text-xs mb-1 block">2015 - L'√àre Moderne</span>
                        <h3 class="text-lg font-semibold dl-text-primary">Kingma & Ba</h3>
                        <p class="dl-text-muted mt-1 text-sm">Publication de l'optimiseur Adam, qui devient rapidement le standard pour entra√Æner les r√©seaux profonds.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- PI√àGES -->
        <section id="pitfalls" class="mb-16 scroll-mt-24">
            <div class="flex items-center gap-3 mb-6">
                <span class="text-3xl">‚ö†Ô∏è</span>
                <h2 class="text-3xl font-bold dl-text-primary">Pi√®ges Classiques</h2>
            </div>

            <div class="grid md:grid-cols-2 gap-6">
                <div class="card-hover dl-card p-6 rounded-xl border">
                    <div class="flex items-center gap-3 mb-4">
                        <div class="w-10 h-10 rounded-lg bg-red-500/20 flex items-center justify-center">üìâ</div>
                        <h3 class="text-lg font-bold dl-text-primary">Vanishing Gradient</h3>
                    </div>
                    <p class="text-sm dl-text-muted mb-4">
                        Dans les r√©seaux tr√®s profonds, les gradients se multiplient couche apr√®s couche. Si chaque d√©riv√©e est &lt; 1, le gradient devient minuscule et les premi√®res couches n'apprennent plus.
                    </p>
                    <div class="text-xs text-blue-400 font-mono">
                        Solutions : ReLU, Batch Normalization, ResNets
                    </div>
                </div>

                <div class="card-hover dl-card p-6 rounded-xl border">
                    <div class="flex items-center gap-3 mb-4">
                        <div class="w-10 h-10 rounded-lg bg-purple-500/20 flex items-center justify-center">üï≥Ô∏è</div>
                        <h3 class="text-lg font-bold dl-text-primary">Minimums Locaux</h3>
                    </div>
                    <p class="text-sm dl-text-muted mb-4">
                        La descente de gradient peut rester coinc√©e dans un "creux" qui n'est pas le minimum global. En pratique, en haute dimension, c'est moins probl√©matique qu'on ne le pensait.
                    </p>
                    <div class="text-xs text-purple-400 font-mono">
                        Solutions : Momentum, Adam, Learning Rate Scheduling
                    </div>
                </div>

                <div class="card-hover dl-card p-6 rounded-xl border">
                    <div class="flex items-center gap-3 mb-4">
                        <div class="w-10 h-10 rounded-lg bg-yellow-500/20 flex items-center justify-center">üìö</div>
                        <h3 class="text-lg font-bold dl-text-primary">Overfitting</h3>
                    </div>
                    <p class="text-sm dl-text-muted mb-4">
                        Le r√©seau "apprend par c≈ìur" les donn√©es d'entra√Ænement au lieu de g√©n√©raliser. Il performe bien sur les donn√©es vues, mais mal sur les nouvelles.
                    </p>
                    <div class="text-xs text-yellow-500 font-mono">
                        Solutions : Early Stopping, Dropout, Regularization
                    </div>
                </div>

                <div class="card-hover dl-card p-6 rounded-xl border">
                    <div class="flex items-center gap-3 mb-4">
                        <div class="w-10 h-10 rounded-lg bg-orange-500/20 flex items-center justify-center">üí•</div>
                        <h3 class="text-lg font-bold dl-text-primary">Exploding Gradient</h3>
                    </div>
                    <p class="text-sm dl-text-muted mb-4">
                        L'inverse du vanishing : les gradients deviennent √©normes et font "exploser" les poids vers l'infini.
                    </p>
                    <div class="text-xs text-orange-400 font-mono">
                        Solutions : Gradient Clipping, Careful Initialization
                    </div>
                </div>
            </div>
        </section>

    </main>

    <footer class="dl-footer border-t py-8 text-center text-sm">
        <p>Deep Learning pour l'impatient ‚Äî Slide 7/8</p>
    </footer>

</body>
</html>
